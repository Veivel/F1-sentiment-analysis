{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Gathering & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook gathers tweets using Twitter's API and\n",
    "cleans them to prepare for the second act."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- links still exist (i saw it on last word)\n",
    "- non-english still eixst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import IPython\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Emoji library (for demojization)\n",
    "import emoji\n",
    "emojis = list(emoji.EMOJI_DATA.keys())\n",
    "\n",
    "# Language Detection\n",
    "import spacy\n",
    "import spacy_fastlang # is used\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "# Stopwords to remove\n",
    "from nltk.corpus import stopwords as sw\n",
    "stopwords = sw.words('English')\n",
    "stopwords.remove('not')\n",
    "\n",
    "# Keyword\n",
    "keyword = \"leclerc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Private keys & secrets to authorize Tweepy client\n",
    "acc = open(\"access.txt\", \"r\")\n",
    "\n",
    "# consumer key & secret\n",
    "api_key = acc.readline()\n",
    "api_secret = acc.readline()\n",
    "\n",
    "# access token/key & secret\n",
    "access_key = acc.readline()\n",
    "access_secret = acc.readline()\n",
    "\n",
    "# bearer token\n",
    "bearer = acc.readline()\n",
    "\n",
    "client = tweepy.Client(\n",
    "    bearer_token        = bearer,\n",
    "    access_token        = access_key,\n",
    "    access_token_secret = access_secret,\n",
    "    consumer_key        = api_key,\n",
    "    consumer_secret     = api_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_emojis(lst):\n",
    "    res = []\n",
    "    for sentence in lst:\n",
    "        clean = emoji.demojize(sentence)\n",
    "        res.append(clean)\n",
    "    return res\n",
    "        \n",
    "\n",
    "def clean_sentences(lst):\n",
    "    ''' Cleans up a list of tweets.\n",
    "    Removes: Links, tags, retweets, emojis'''\n",
    "    res = []\n",
    "    for sentence in lst:\n",
    "        sentence = sentence.lower()     # lower-cases sentence\n",
    "        \n",
    "        # sentence = emoji.demojize(sentence, delimiters=('  ', '  '))\n",
    "        \n",
    "        words = sentence.split(' ')\n",
    "        new_words = copy.deepcopy(words)\n",
    "        \n",
    "        for word in words: # Iterates through each word in the tweet\n",
    "            if len(word) == 0:\n",
    "                new_words.remove(word)\n",
    "            elif word[:4] == \"http\":    # Removes links\n",
    "                new_words.remove(word)\n",
    "            elif word[0] == \"@\":        # Removes tags\n",
    "                new_words.remove(word)\n",
    "            elif word in stopwords:     # Removes stopwords\n",
    "                new_words.remove(word)\n",
    "            elif word[:2] == \"rt\":      # Removes retweets\n",
    "                new_words.remove(word)\n",
    "            elif word[:2] == \"\\n\":      # Removes line breaks\n",
    "                new_words.remove(word)\n",
    "            elif word in emojis:        # Out-right removes emojis. This decision\n",
    "                new_words.remove(word)  #  was made because how specific all of the\n",
    "                                        # demojized emoji names were. and the delimiters didnt work well.\n",
    "                                        \n",
    "        sentence = \" \".join(new_words)\n",
    "        # sentence = re.sub(string.punctuation, '', sentence) # Removes punctuation\n",
    "        for chr in string.punctuation:\n",
    "            sentence = sentence.replace(chr, \"\")\n",
    "        \n",
    "        res.append(sentence)\n",
    "    return res\n",
    "\n",
    "def clean_words(lst):\n",
    "    ''' Removes: empty spaces'''\n",
    "    res = []\n",
    "    for sentence in lst:\n",
    "        words = sentence.split(' ')\n",
    "        new_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            for char in word: # iterates through each character in the tweet\n",
    "                if char == \" \":\n",
    "                    word.replace(char, \"\")\n",
    "            new_words.append(word)\n",
    "            \n",
    "        sentence = \" \".join(new_words)\n",
    "        res.append(sentence) \n",
    "    return res\n",
    "    \n",
    "def remove_non_english(lst):\n",
    "    ''' Removes all content that is NOT english from a list of tweets'''\n",
    "    langs = []\n",
    "    res = copy.deepcopy(lst)\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(\"language_detector\")\n",
    "    \n",
    "    for item in lst: # iterates through each sentence\n",
    "        doc = nlp(item)\n",
    "        lang = doc._.language\n",
    "        if lang != 'en':\n",
    "            res.remove(item)        # removes non-english sentences\n",
    "        langs.append(lang)\n",
    "    lang_labels['language'] = langs # secondary side-effect\n",
    "    return res\n",
    "\n",
    "def stem(lst):\n",
    "    '''Stems words to use basic stem word (e.g turn instead of turning)'''\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    res = []\n",
    "    for sentence in lst:\n",
    "        new_sentence = []\n",
    "        for word in sentence.split():\n",
    "            new_word = ps.stem(word)\n",
    "            new_sentence.append(new_word)\n",
    "        new_sentence = \" \".join(new_sentence)\n",
    "        res.append(new_sentence)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining & Pipelining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of the monthly limit on tweet data, we should re-use previously obtained tweets.\n",
    "GET_NEW_TWEETS = False\n",
    "\n",
    "if GET_NEW_TWEETS:\n",
    "    tweets = set()\n",
    "    recent = client.search_recent_tweets(query=keyword, max_results=100)\n",
    "    \n",
    "    for item in recent[0]:\n",
    "        tweets.add(item.text)\n",
    "    data = list(tweets)\n",
    "    pd.Series(data).to_csv('raw.csv', index=False)\n",
    "else:\n",
    "    tweets = pd.read_csv('raw.csv')\n",
    "    data = list(tweets['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "lang_labels = pd.DataFrame(data, columns=['tweet content'])\n",
    "\n",
    "data = handle_emojis(data)\n",
    "data = clean_sentences(data)\n",
    "# data = clean_words(data) # no need\n",
    "data = remove_non_english(data)\n",
    "data = stem(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(data).to_csv('clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Training Data (labeling sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This requires user interaction to label whether a tweet is positive, negative or neutral.\n",
    "<br> (and alternatively, if a tweet is simply gibberish and should be removed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ===== ===== ===== ===== ===== ===== ===== \n",
      "Label the sentiment of the following tweets about an f1 driver / team: leclerc\n",
      "1 : negative sentiment\n",
      "2 : positive sentiment\n",
      "0 : neutral / no sentiment\n",
      "X : erase this tweet\n",
      "STOP : stop program\n",
      "\n",
      "Tweet:\n",
      "charl leclerc brace bumpi ride canadian grand prix earli championship lead becom 34point deficit due engin failur ferrari strategi mistak httpstcosnpy3febvd\n",
      "===== ===== ===== ===== ===== ===== ===== ===== \n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3259: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# TODO: how to handle negation?\n",
    "\n",
    "labels = []\n",
    "data_copy = copy.deepcopy(data)\n",
    "for sentence in data_copy:\n",
    "    if sentence == \"\": # TODO: this should be handled elsewhere....\n",
    "        data.remove(sentence)\n",
    "        continue\n",
    "    inputValid = False\n",
    "    \n",
    "    print(\"===== \"*8)\n",
    "    print(f\"Label the sentiment of the following tweets about an f1 driver / team: {keyword}\")\n",
    "    print(\"1 : negative sentiment\\n2 : positive sentiment\\n0 : neutral / no sentiment\\nX : erase this tweet\\nSTOP : stop program\")\n",
    "    print(\"\\nTweet:\")\n",
    "    print(sentence)\n",
    "    print(\"===== \"*8)\n",
    "    \n",
    "    while (not inputValid):\n",
    "        inp = input(\"Input: \")\n",
    "        if inp in [\"0\", \"1\", \"2\"]:\n",
    "            inputValid = True\n",
    "            labels.append(int(inp))\n",
    "        elif inp.lower() == \"x\":\n",
    "            inputValid = True\n",
    "            data.remove(sentence)\n",
    "        elif inp == \"STOP\":\n",
    "            inputValid = True\n",
    "            IPython.sys.exit()\n",
    "        else:\n",
    "            continue\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(data=[data, labels]).transpose()\n",
    "final.columns = ['tweet content', 'sentiment']\n",
    "\n",
    "final.to_csv('out.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
