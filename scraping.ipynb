{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import IPython\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Emoji library (for demojization)\n",
    "import emoji\n",
    "emojis = list(emoji.EMOJI_DATA.keys())\n",
    "\n",
    "# Language Detection\n",
    "import spacy\n",
    "import spacy_fastlang # is used\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "# Stopwords to remove\n",
    "from nltk.corpus import stopwords as sw\n",
    "stopwords = sw.words('English')\n",
    "stopwords.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Private keys & secrets to authorize Tweepy client\n",
    "acc = open(\"access.txt\", \"r\")\n",
    "\n",
    "# consumer key & secret\n",
    "api_key = acc.readline()\n",
    "api_secret = acc.readline()\n",
    "\n",
    "# access token/key & secret\n",
    "access_key = acc.readline()\n",
    "access_secret = acc.readline()\n",
    "\n",
    "# bearer token\n",
    "bearer = acc.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_emojis(lst):\n",
    "    res = []\n",
    "    for sentence in lst:\n",
    "        clean = emoji.demojize(sentence)\n",
    "        res.append(clean)\n",
    "    return res\n",
    "        \n",
    "\n",
    "def clean_sentences(lst):\n",
    "    ''' Cleans up a list of tweets.\n",
    "    Removes: Links, tags, retweets, emojis'''\n",
    "    res = []\n",
    "    for sentence in lst:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = emoji.demojize(sentence)\n",
    "        \n",
    "        words = sentence.split(' ')\n",
    "        new_words = copy.deepcopy(words)\n",
    "        \n",
    "        for word in words:\n",
    "            if len(word) == 0:\n",
    "                new_words.remove(word)\n",
    "            elif word[:4] == \"http\":\n",
    "                new_words.remove(word)\n",
    "            elif word[0] == \"@\":\n",
    "                new_words.remove(word)\n",
    "            elif word in stopwords:\n",
    "                new_words.remove(word)\n",
    "            elif word[:2] == \"rt\":\n",
    "                new_words.remove(word)\n",
    "            elif word[:2] == \"\\n\":\n",
    "                new_words.remove(word)\n",
    "        \n",
    "        sentence = \" \".join(new_words)\n",
    "        res.append(sentence)\n",
    "    return res\n",
    "\n",
    "def clean_words(lst):\n",
    "    ''' Removes: punctuation and unwanted characters'''\n",
    "    res = []\n",
    "    for sentence in lst:\n",
    "        words = sentence.split(' ')\n",
    "        new_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            for char in word:\n",
    "                if char in string.punctuation:\n",
    "                    word.replace(char, \"\")\n",
    "                if char == \" \":\n",
    "                    word.replace(char, \"\")\n",
    "            new_words.append(word)\n",
    "            \n",
    "        sentence = \" \".join(new_words)\n",
    "        res.append(sentence) \n",
    "    return res\n",
    "    \n",
    "def remove_all_non_en(lst):\n",
    "    ''' Removes all content that is NOT english from a list of tweets'''\n",
    "    langs = []\n",
    "    res = copy.deepcopy(lst)\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(\"language_detector\")\n",
    "    \n",
    "    for item in lst:\n",
    "        doc = nlp(item)\n",
    "        lang = doc._.language\n",
    "        if lang != 'en':\n",
    "            res.remove(item)\n",
    "        langs.append(lang)\n",
    "    lang_labels['language'] = langs\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(\n",
    "    bearer_token=bearer,\n",
    "    access_token=access_key,\n",
    "    access_token_secret=access_secret,\n",
    "    consumer_key=api_key,\n",
    "    consumer_secret=api_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_NEW_TWEETS = False\n",
    "\n",
    "if GET_NEW_TWEETS:\n",
    "    tweets = set()\n",
    "    recent = client.search_recent_tweets(query=\"leclerc\", max_results=100)\n",
    "    \n",
    "    for item in recent[0]:\n",
    "        tweets.add(item.text)\n",
    "    data = list(tweets)\n",
    "    pd.Series(data).to_csv('raw.csv', index=False)\n",
    "else:\n",
    "    tweets = pd.read_csv('raw.csv')\n",
    "    data = list(tweets['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "data = handle_emojis(data)\n",
    "data = clean_words(data)\n",
    "data = clean_sentences(data)\n",
    "# TODO: dont forget stemming\n",
    "\n",
    "lang_labels = pd.DataFrame(data, columns=['tweet content'])\n",
    "data = remove_all_non_en(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(data).to_csv('clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "Label the sentiment of the following tweets about an f1 driver / team.\n",
      "1 = negative sentiment\n",
      "2 = positive sentiment\n",
      "0 = neutral / no sentiment\n",
      "X = erase this tweet\n",
      "STOP = stop program\n",
      "\n",
      "= = = = = = = = = = = = = = = = = = = = = = = = \n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3259: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Driver / Team sentiment\n",
    "# Final pipeline before actual NLP\n",
    "\n",
    "# TODO: how to handle negation?\n",
    "\n",
    "labels = []\n",
    "data_copy = copy.deepcopy(data)\n",
    "for sentence in data_copy:\n",
    "    if sentence == \"\": # TODO: this should be handled elsewhere....\n",
    "        data.remove(sentence)\n",
    "        continue\n",
    "    inputValid = False\n",
    "    \n",
    "    print(\"=\"*24)\n",
    "    print(\"Label the sentiment of the following tweets about an f1 driver / team.\")\n",
    "    print(\"1 = negative sentiment\\n2 = positive sentiment\\n0 = neutral / no sentiment\\nX = erase this tweet\\nSTOP = stop program\")\n",
    "    print(sentence)\n",
    "    print(\"= \"*24)\n",
    "    \n",
    "    while (not inputValid):\n",
    "        inp = input(\"Input: \")\n",
    "        if inp in [\"0\", \"1\", \"2\"]:\n",
    "            inputValid = True\n",
    "            labels.append(int(inp))\n",
    "        elif inp.lower() == \"x\":\n",
    "            inputValid = True\n",
    "            data.remove(sentence)\n",
    "        elif inp == \"STOP\":\n",
    "            inputValid = True\n",
    "            IPython.sys.exit()\n",
    "        else:\n",
    "            continue\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(data=[data, labels]).transpose()\n",
    "final.columns = ['tweet content', 'sentiment']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
