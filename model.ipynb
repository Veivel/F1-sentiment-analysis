{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import IPython\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Emoji library (for demojization)\n",
    "import emoji\n",
    "emojis = list(emoji.EMOJI_DATA.keys())\n",
    "\n",
    "# Language Detection\n",
    "import spacy\n",
    "import spacy_fastlang # is used\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "# Stopwords to remove\n",
    "from nltk.corpus import stopwords as sw\n",
    "stopwords = sw.words('English')\n",
    "stopwords.remove('not')\n",
    "\n",
    "from sklearn import feature_extraction, model_selection, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import naive_bayes, ensemble, tree\n",
    "\n",
    "import xgboost\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_emojis(lst):\n",
    "    res = []\n",
    "    for sentence in lst:\n",
    "        words = sentence.split(' ')\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            for char in word:\n",
    "                if char in emojis:\n",
    "                    word.replace(char, \"\")\n",
    "            new_words.append(word)\n",
    "        sentence = ' '.join(new_words)\n",
    "        res.append(sentence)\n",
    "    return res\n",
    "        \n",
    "\n",
    "def clean_sentences(lst):\n",
    "    ''' Cleans up a list of tweets.\n",
    "    Removes: Links, tags, retweets, emojis'''\n",
    "    res = []\n",
    "    for sentence in lst:\n",
    "        sentence = sentence.lower()     # lower-cases sentence\n",
    "        \n",
    "        words = sentence.split(' ')\n",
    "        new_words = copy.deepcopy(words)\n",
    "        \n",
    "        for word in words: # Iterates through each word in the tweet\n",
    "            if len(word) == 0:\n",
    "                new_words.remove(word)\n",
    "            elif word[:4] == \"http\":    # Removes links\n",
    "                new_words.remove(word)\n",
    "            elif word[0] == \"@\":        # Removes tags\n",
    "                new_words.remove(word)\n",
    "            elif word in stopwords:     # Removes stopwords\n",
    "                new_words.remove(word)\n",
    "            elif word[:2] == \"rt\":      # Removes retweets\n",
    "                new_words.remove(word)\n",
    "            elif word[:2] == \"\\n\":      # Removes line breaks\n",
    "                new_words.remove(word)\n",
    "                                        \n",
    "        sentence = \" \".join(new_words)\n",
    "        # sentence = re.sub(string.punctuation, '', sentence) # Removes punctuation\n",
    "        for chr in string.punctuation:\n",
    "            sentence = sentence.replace(chr, \"\")\n",
    "        \n",
    "        res.append(sentence)\n",
    "    return res\n",
    "\n",
    "def clean_words(lst):\n",
    "    ''' Removes: empty spaces'''\n",
    "    res = []\n",
    "    for sentence in lst:\n",
    "        words = sentence.split(' ')\n",
    "        new_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            for char in word: # iterates through each character in the tweet\n",
    "                if char == \" \":\n",
    "                    word.replace(char, \"\")\n",
    "            new_words.append(word)\n",
    "            \n",
    "        sentence = \" \".join(new_words)\n",
    "        res.append(sentence) \n",
    "    return res\n",
    "    \n",
    "def remove_non_english(lst):\n",
    "    ''' Removes all content that is NOT english from a list of tweets'''\n",
    "    langs = []\n",
    "    res = copy.deepcopy(lst)\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(\"language_detector\")\n",
    "    \n",
    "    for item in lst: # iterates through each sentence\n",
    "        doc = nlp(item)\n",
    "        lang = doc._.language\n",
    "        if lang != 'en':\n",
    "            res.remove(item)        # removes non-english sentences\n",
    "        langs.append(lang)\n",
    "    # lang_labels['language'] = langs # secondary side-effect\n",
    "    return res\n",
    "\n",
    "def stem(lst):\n",
    "    '''Stems words to use basic stem word (e.g turn instead of turning)'''\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    res = []\n",
    "    for sentence in lst:\n",
    "        new_sentence = []\n",
    "        for word in sentence.split():\n",
    "            new_word = ps.stem(word)\n",
    "            new_sentence.append(new_word)\n",
    "        new_sentence = \" \".join(new_sentence)\n",
    "        res.append(new_sentence)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('data/labeled/sample.csv', header=0)\n",
    "data = copy.deepcopy(list(raw['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_labels = pd.DataFrame(data, columns=['tweet content'])\n",
    "\n",
    "data = handle_emojis(data)\n",
    "data = clean_sentences(data)\n",
    "# data = remove_non_english(data)\n",
    "data = stem(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['text'] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORECAST_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "X = cv.fit_transform(raw['text']).toarray()\n",
    "y = raw['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classifiers = {\n",
    "    'xgb': xgboost.XGBClassifier(),\n",
    "    'gaussian_nb': naive_bayes.GaussianNB(),\n",
    "    'multi_nb': naive_bayes.MultinomialNB(),\n",
    "    'rforest': ensemble.RandomForestClassifier(),\n",
    "    'entropy_rforest': ensemble.RandomForestClassifier(criterion='entropy'),\n",
    "    'decision_tree': tree.DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "choice = 'gaussian_nb'\n",
    "\n",
    "classifier = dict_classifiers[choice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of xgb: 0.9811738648947952\n",
      "Score of gaussian_nb: 0.8638981173864895\n",
      "Score of multi_nb: 0.9337763012181617\n",
      "Score of rforest: 0.9811738648947952\n",
      "Score of entropy_rforest: 0.9811738648947952\n",
      "Score of decision_tree: 0.9765227021040974\n"
     ]
    }
   ],
   "source": [
    "if not FORECAST_MODE:\n",
    "    for name in dict_classifiers:\n",
    "        clf = dict_classifiers[name]\n",
    "        scores = scores = model_selection.cross_val_score(clf, X, y, cv=5)\n",
    "        score = np.average(scores)\n",
    "        print(f\"Score of {name}: {score}\")\n",
    "else:\n",
    "    # Export stuff\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
