{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook cleans the tweets to prepare for the third phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import IPython\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Emoji library (for demojization)\n",
    "import emoji\n",
    "emojis = list(emoji.EMOJI_DATA.keys())\n",
    "\n",
    "# Language Detection\n",
    "import spacy\n",
    "import spacy_fastlang # is used\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "# Stopwords to remove\n",
    "from nltk.corpus import stopwords as sw\n",
    "stopwords = sw.words('English')\n",
    "stopwords.remove('not')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_emojis(lst):\n",
    "    res = []\n",
    "    for sentence in lst:\n",
    "        words = sentence.split(' ')\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            for char in word:\n",
    "                if char in emojis:\n",
    "                    word.replace(char, \"\")\n",
    "            new_words.append(word)\n",
    "        sentence = ' '.join(new_words)\n",
    "        res.append(sentence)\n",
    "    return res\n",
    "        \n",
    "\n",
    "def clean_sentences(lst):\n",
    "    ''' Cleans up a list of tweets.\n",
    "    Removes: Links, tags, retweets, emojis'''\n",
    "    res = []\n",
    "    for sentence in lst:\n",
    "        sentence = sentence.lower()     # lower-cases sentence\n",
    "        \n",
    "        words = sentence.split(' ')\n",
    "        new_words = copy.deepcopy(words)\n",
    "        \n",
    "        for word in words: # Iterates through each word in the tweet\n",
    "            if len(word) == 0:\n",
    "                new_words.remove(word)\n",
    "            elif word[:4] == \"http\":    # Removes links\n",
    "                new_words.remove(word)\n",
    "            elif word[0] == \"@\":        # Removes tags\n",
    "                new_words.remove(word)\n",
    "            elif word in stopwords:     # Removes stopwords\n",
    "                new_words.remove(word)\n",
    "            elif word[:2] == \"rt\":      # Removes retweets\n",
    "                new_words.remove(word)\n",
    "            elif word[:2] == \"\\n\":      # Removes line breaks\n",
    "                new_words.remove(word)\n",
    "                                        \n",
    "        sentence = \" \".join(new_words)\n",
    "        # sentence = re.sub(string.punctuation, '', sentence) # Removes punctuation\n",
    "        for chr in string.punctuation:\n",
    "            sentence = sentence.replace(chr, \"\")\n",
    "        \n",
    "        res.append(sentence)\n",
    "    return res\n",
    "\n",
    "def clean_words(lst):\n",
    "    ''' Removes: empty spaces'''\n",
    "    res = []\n",
    "    for sentence in lst:\n",
    "        words = sentence.split(' ')\n",
    "        new_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            for char in word: # iterates through each character in the tweet\n",
    "                if char == \" \":\n",
    "                    word.replace(char, \"\")\n",
    "            new_words.append(word)\n",
    "            \n",
    "        sentence = \" \".join(new_words)\n",
    "        res.append(sentence) \n",
    "    return res\n",
    "    \n",
    "def remove_non_english(lst):\n",
    "    ''' Removes all content that is NOT english from a list of tweets'''\n",
    "    langs = []\n",
    "    res = copy.deepcopy(lst)\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(\"language_detector\")\n",
    "    \n",
    "    for item in lst: # iterates through each sentence\n",
    "        doc = nlp(item)\n",
    "        lang = doc._.language\n",
    "        if lang != 'en':\n",
    "            res.remove(item)        # removes non-english sentences\n",
    "        langs.append(lang)\n",
    "    lang_labels['language'] = langs # secondary side-effect\n",
    "    return res\n",
    "\n",
    "def stem(lst):\n",
    "    '''Stems words to use basic stem word (e.g turn instead of turning)'''\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    res = []\n",
    "    for sentence in lst:\n",
    "        new_sentence = []\n",
    "        for word in sentence.split():\n",
    "            new_word = ps.stem(word)\n",
    "            new_sentence.append(new_word)\n",
    "        new_sentence = \" \".join(new_sentence)\n",
    "        res.append(new_sentence)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('raw copy.txt')\n",
    "raw = f.readlines()\n",
    "\n",
    "data = pd.Series(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "lang_labels = pd.DataFrame(data, columns=['tweet content'])\n",
    "\n",
    "data = handle_emojis(data)\n",
    "data = clean_sentences(data)\n",
    "# data = clean_words(data) # no need\n",
    "data = remove_non_english(data)\n",
    "data = stem(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(data).to_csv('clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Training Data (labeling sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This requires user interaction to label whether a tweet is positive, negative or neutral.\n",
    "<br> (and alternatively, if a tweet is simply gibberish and should be removed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: expecting '}' (1469499442.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [70]\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Label the sentiment of the following tweets about an f1 driver / team: {\"leclerc\"}\")\u001b[0m\n\u001b[0m                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: expecting '}'\n"
     ]
    }
   ],
   "source": [
    "# TODO: how to handle negation?\n",
    "\n",
    "labels = []\n",
    "data_copy = copy.deepcopy(data)\n",
    "for sentence in data_copy:\n",
    "    if sentence == \"\": # TODO: this should be handled elsewhere....\n",
    "        data.remove(sentence)\n",
    "        continue\n",
    "    inputValid = False\n",
    "    \n",
    "    print(\"===== \"*8)\n",
    "    print(f\"Label the sentiment of the following tweets about an f1 driver / team: leclerc\")\n",
    "    print(\"1 : negative sentiment\\n2 : positive sentiment\\n0 : neutral / no sentiment\\nX : erase this tweet\\nSTOP : stop program\")\n",
    "    print(\"\\nTweet:\")\n",
    "    print(sentence)\n",
    "    print(\"===== \"*8)\n",
    "    \n",
    "    while (not inputValid):\n",
    "        inp = input(\"Input: \")\n",
    "        if inp in [\"0\", \"1\", \"2\"]:\n",
    "            inputValid = True\n",
    "            labels.append(int(inp))\n",
    "        elif inp.lower() == \"x\":\n",
    "            inputValid = True\n",
    "            data.remove(sentence)\n",
    "        elif inp == \"STOP\":\n",
    "            inputValid = True\n",
    "            IPython.sys.exit()\n",
    "        else:\n",
    "            continue\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(data=[data, labels]).transpose()\n",
    "final.columns = ['tweet content', 'sentiment']\n",
    "\n",
    "final.to_excel('out.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
